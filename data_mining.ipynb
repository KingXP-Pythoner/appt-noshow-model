{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRN0AsMsAEnz"
      },
      "source": [
        "# Data Mining/Machine Learning Project: Medical Appointments - No Show\n",
        "\n",
        "## Goals\n",
        "1. Given a set of attributes/factors, predict if a person will miss their appointment or not.\n",
        "2. Determine what factors contribute the most to a person missing their appointment.\n",
        "3. Compare the performance of the 2 data mining/analysis methods implemented for this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B5qag8eBvod"
      },
      "source": [
        "##I. Business Understanding\n",
        "\n",
        "Missed appointments are costly on the medical institutions. Therefore, understanding the factors that cause no-shows are vital in the search for potential solutions to these problems. Having the information about the data set have the following benefits:\n",
        "\n",
        "1. Hospital can intelligently send more reminders to patients at a higher risk of missing appointments.\n",
        "2. Understand if the reminder methods (in this case: SMS) are effective or not, and make changes as necessary to the strategies.\n",
        "3. Inform appointment management/scheduling strategy. (More on the day or more routine appointments?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfOJoig3HljX"
      },
      "source": [
        "## II. Data Understanding\n",
        "### Dataset:\n",
        "The dataset contains information about medical appointments and has 14 variables (PatientId, AppointmentID, Gender, DateScheduled, AppointmentDate, Age, Neighborhood, Scholarship, Hypertension, Diabetes, Alcoholism, Handicap, SMSReceived, NoShow).\n",
        "\n",
        "### Tasks:\n",
        "\n",
        "Explore the dataset to understand its structure, size, and features.\n",
        "Check for missing values, outliers, and data types.\n",
        "Understand the distribution of the target variable (NoShow).\n",
        "Explore and analyze the relationships between features and the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZAmQYfwNg4g"
      },
      "outputs": [],
      "source": [
        "# Load the required libraries\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "plt.style.use('fivethirtyeight')\n",
        "pd.set_option('display.width', 1000)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data into a pandas dataframe\n",
        "df = pd.read_csv('dataset.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Size, Dimensionality, Size, Data types\n",
        "The dataset provided by [source] has a 110527 x 14 (mxn) dimensionality. We can identify the following columns and their data types (as nominal, ordinal, or continuous):\n",
        "1. PatientId: nominal\n",
        "2. AppointmentID: nominal\n",
        "3. Gender: nominal\n",
        "4. ScheduledDay: date type\n",
        "5. AppointmentDay: date type\n",
        "6. Age: continuous\n",
        "7. Neighbourhood: nominal\n",
        "8. Scholarship: nominal\n",
        "9. Hypertension: nominal\n",
        "10. Diabetes: nominal\n",
        "11. Alcoholism: nominal\n",
        "12. Handcap: nominal\n",
        "13. SMS_received: nominal\n",
        "14. No-show: nominal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset shape\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First 5 rows of the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset has 14 columns or characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List columns in the dataset\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Duplication check\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataframe overall information\n",
        "The dataset has no missing values across all rows and columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df.drop(columns=['AppointmentID'], inplace=True, axis=1)\n",
        "df.drop_duplicates(['PatientId','No-show'], inplace = True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## rename columns to fix typos and use pythonic naming conventions\n",
        "column_rename_dict = {}\n",
        "for column in df.columns:\n",
        "    column_rename_dict[column] = column.lower().replace(' ', '_')\n",
        "    if '-' in column_rename_dict[column]:\n",
        "        column_rename_dict[column] = column_rename_dict[column].replace('-', '_')\n",
        "\n",
        "column_rename_dict['Hipertension'] = 'hypertension'\n",
        "column_rename_dict['Handcap'] = 'handicap'\n",
        "column_rename_dict['AppointmentDay'] = 'appointment_day'\n",
        "column_rename_dict['ScheduledDay'] = 'scheduled_day'\n",
        "column_rename_dict['PatientId'] = 'patient_id'\n",
        "\n",
        "df.rename(columns=column_rename_dict, inplace=True)\n",
        "# Check\n",
        "df.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Descriptive Statistics\n",
        "1. Minimum age is -1 which is not possible.\n",
        "2. Scholarship, Hypertension, Diabetes are binary for all rows. But Handicap has a max value of 4. This could mean this attribute should be binary and these >1 values are errors or it means the number of handicaps the patient had. The description provided from the source via Kaggle states it should be represented as True or False, but the Discussions revealed the attribute is the number of handicaps the patient has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df.select_dtypes(exclude='object').columns.drop([\"patient_id\"])].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols_no_age = df.select_dtypes(exclude='object').columns.drop(['patient_id', 'age'])\n",
        "\n",
        "# Concatenate the percentage distribution data for all columns\n",
        "perc_dist = pd.concat([pd.DataFrame({f\"{column} value\": df[column].value_counts(normalize=True).index,\n",
        "                                                f\"{column} percentage %\": (df[column].value_counts(normalize=True) * 100).round(4).values})\n",
        "                                  for column in num_cols_no_age], axis=1)\n",
        "perc_dist = perc_dist.fillna(0)\n",
        "\n",
        "perc_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning\n",
        "The goal is to remove anomalies from the data to develop data quality. Since in the descriptive statistics check an anomaly was observed in both the age and handicap columns, data cleaning operation can be performed in these columns. We can also ensure date type columns are converted correctly to datetime.\n",
        "### Steps:\n",
        "1. Remove the row will the age = -1. Manual removal is done here as it is simply 1 record with this issue and will not significantly impact the age column in correlation to the target variable for data modeling.\n",
        "2. Convert scheduled_day and appointment_day columns to datetime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove rows with negative age\n",
        "df.query(\"age == -1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if there are enough rows in the dataset for ROMÃO. If that's the case, the loss is negligible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df['neighbourhood'] ==  \"ROMÃO\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop row with negative age\n",
        "from matplotlib import axis\n",
        "\n",
        "\n",
        "\n",
        "df.drop(df[df['age']< 0].index, inplace = True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "print(df.tail())\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the scheduled_day and appointment_day columns to datetime\n",
        "df['scheduled_day'] = pd.to_datetime(df['scheduled_day'])\n",
        "df['appointment_day'] = pd.to_datetime(df['appointment_day'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nominal_columns = df.select_dtypes(include='object').columns\n",
        "numerical_columns =df.select_dtypes(exclude='object').columns\n",
        "\n",
        "nominal_cols_list = nominal_columns.tolist()\n",
        "num_cols_list = numerical_columns.tolist()\n",
        "numerical_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df.columns.drop(['patient_id', 'appointment_day', 'scheduled_day'])].hist(figsize=(16,8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Age Group Distribution\n",
        "From plotting age distribution on a bar chart, baby (0 years) patients have the most frequency. The distribution is slightly left skewed meaning only a minority sample of the patient population in the dataframe were of the senior/elderly population."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Discretize the Age column into bins\n",
        "age_bins = [0, 18, 30, 45, 60, 75, 100]\n",
        "age_labels = ['0-17', '18-29', '30-44', '45-59', '60-74', '75-100']\n",
        "age_group_df = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)\n",
        "\n",
        "# Calculate age group counts and percentages\n",
        "age_group_counts = age_group_df.value_counts().sort_index()\n",
        "age_group_percentages = (age_group_counts / age_group_counts.sum()) * 100\n",
        "\n",
        "# Prepare the data for plotting\n",
        "plot_data = pd.DataFrame({'AgeGroup': age_group_counts.index, 'Count': age_group_counts.values, 'Percentage': age_group_percentages.values})\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(data=plot_data, x='AgeGroup', y='Count', palette='pastel', hue='AgeGroup', dodge=False)\n",
        "plt.legend([],[], frameon=False)  # Hide the legend\n",
        "\n",
        "# Add percentages on top of bars\n",
        "for i, (count, percentage) in enumerate(zip(plot_data['Count'], plot_data['Percentage'])):\n",
        "    plt.text(i, count, f'{percentage:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.title('Age Group Distribution')\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Number of Patients')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show = df['no_show'] == 'No'\n",
        "no_show = df['no_show'] == 'Yes'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot histograms for age based on attendance\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Histogram for age of patients who showed up\n",
        "plt.hist(df['age'][show], bins=18, color='green', alpha=0.5,  label='Showed up')\n",
        "# Histogram for age of patients who didn't show up\n",
        "plt.hist(df['age'][no_show], bins=18, alpha=0.5, color='red', label='No show')\n",
        "\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Number of Patients')\n",
        "plt.title('Attendance by Age')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of attendance between genders (Male and Female)\n",
        "\n",
        "The analysis shows that 64.9% of females attended their appointments versus 35.1% of males, and 65.39% of females did not attend versus 34.61% of males. This indicates that while the dataset is skewed towards females, gender here is not a strong predictor of no-show behavior due to the similar percentage distributions across both attendance and no-shows. Consequently, to achieve the project's aim of improving attendance rates, it is crucial to explore other variables such as age, medical conditions, and the impact of SMS reminders, which may provide stronger correlations and insights into patient attendance patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "gender_showed = df[show]['gender'].value_counts(normalize=True)\n",
        "gender_no_show = df[no_show][\"gender\"].value_counts(normalize=True)\n",
        "\n",
        "colors = ['lightgreen', 'lightcoral']\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "gender_showed.plot(kind='pie', autopct='%1.1f%%',ax=axes[0], colors=colors)        \n",
        "axes[0].set_title('Percentage of gender that attended', fontdict={'fontsize':12})\n",
        "axes[0].set_ylabel('')\n",
        "\n",
        "gender_no_show.plot(kind='pie', autopct='%1.1f%%',ax=axes[1], colors=colors) \n",
        "axes[1].set_title('Percentage of gender that did not show up', fontdict={'fontsize': 12})\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.show()       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of attendance by chronic disease\n",
        "(Note to self): Get reference for alcoholism definition as a chronic disease.\n",
        "In exploring the correlation between chronic diseases and appointment attendance, our objective is to understand whether patients with chronic conditions may demonstrate distinct attendance patterns compared to those without such ailments. The analysis unveils a noticeable contrast in attendance rates, with 82.23% of patients with chronic diseases attending appointments versus 79.09% of those without. Conversely, 17.77% of patients with chronic diseases missed appointments, while 20.91% of those without chronic diseases did.\n",
        "\n",
        "This disparity, a 3.14% difference in attendance rates, although may be thought of being relatively small, slightly suggests that ongoing health management may influence attendance behavior, providing insights for healthcare providers to tailor interventions and enhance appointment adherence across patient groups. However, the scale of this influence may not be determined yet as the data collection occured in a short time period. A longer time frame collection may yield better clarity in understanding this influence. But for the goal of the data exploration and modeling, chronic diseases such as hypertension, diabetes and alcoholism do not show a noteworthy correlation with appointment adherence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new column to indicate if a patient has any chronic disease\n",
        "dfc = df.copy(deep=True)\n",
        "dfc['has_chronic_disease'] = dfc[['hypertension', 'diabetes', 'alcoholism']].sum(axis=1) > 0\n",
        "\n",
        "# Calculate the counts of no-shows and shows for patients with and without chronic diseases\n",
        "comparison = dfc.groupby(['has_chronic_disease', 'no_show']).size().unstack().fillna(0)\n",
        "\n",
        "# Plot the bar chart\n",
        "ax = comparison.plot(kind='bar', stacked=True, color=['lightgreen', 'lightcoral'], figsize=(9, 7))\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Chronical Disease Status')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Attendance Comparison: Patients with and without Chronic Diseases')\n",
        "ax.set_xticklabels(['No Chronic Disease', 'Has Chronic Disease'], rotation=0)\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the counts of no-shows and shows for patients with and without chronic diseases\n",
        "attendance_comparison = dfc.groupby(['has_chronic_disease', 'no_show']).size().unstack().fillna(0)\n",
        "\n",
        "# Calculate percentages\n",
        "attendance_percentages = attendance_comparison.div(attendance_comparison.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Prepare data for printing as a table\n",
        "table_data = [\n",
        "    [\"Chronic Disease\", \"No-show\", \"Show\"],\n",
        "    [False, attendance_comparison.loc[False, 'Yes'], attendance_comparison.loc[False, 'No']],\n",
        "    [True, attendance_comparison.loc[True, 'Yes'], attendance_comparison.loc[True, 'No']],\n",
        "]\n",
        "\n",
        "# Print table headers\n",
        "print(\"Counts of Attendance for Patients with and without Chronic Diseases:\")\n",
        "# Print table rows\n",
        "for row in table_data:\n",
        "    print(\"{:<17} | {:<7} | {:<5}\".format(*row))\n",
        "\n",
        "print(\"\\nPercentages of Attendance for Patients with and without Chronic Diseases:\")\n",
        "# Print percentages\n",
        "print(attendance_percentages.round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attendance comparison based on SMS received\n",
        "In analyzing the correlation between SMS reception and appointment attendance, our aim is to discern whether patients who receive SMS reminders exhibit different attendance behavior compared to those who don't. The results reveal a notable difference in attendance rates: 83.30% of patients who did not receive an SMS reminder attended their appointments, while 16.70% did not. The discrepancy show that sending SMS reminders actually had an opposite outcome of the expectation that the reminders would improve attendance. However, we need to investigate how same-day appointments contributes to these findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='sms_received', hue='no_show', data=df, palette='pastel')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('SMS Received', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.title('Attendance comparison based on SMS Reception', fontsize=14)\n",
        "\n",
        "# Show plot\n",
        "plt.legend(title='Attended', labels=['Yes', 'No'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the percentages of show and noshow instances for each category of SMS reception\n",
        "perc_sms_show = df.groupby('sms_received')['no_show'].value_counts(normalize=True)[:, 'No'] * 100\n",
        "perc_sms_noshow = df.groupby('sms_received')['no_show'].value_counts(normalize=True)[:, 'Yes'] * 100\n",
        "\n",
        "# Prepare data for printing as a table\n",
        "table_data = []\n",
        "for sms_received, show_percentage, noshow_percentage in zip(perc_sms_show.index, perc_sms_show.values, perc_sms_noshow.values):\n",
        "    table_data.append([sms_received, show_percentage, noshow_percentage])\n",
        "\n",
        "# Print table headers\n",
        "print(\"SMS Received | Show Percentage | Noshow Percentage\")\n",
        "# Print table rows\n",
        "for row in table_data:\n",
        "    print(\"{:<12} | {:<15.2f}% | {:<15.2f}%\".format(*row))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Same day appointments statistics\n",
        "Roughly 35% of all appointments recorded were same-day appointments. This distribution is significant enough to influence the results gathered earlier. Therefore, it is necessary to filter out same-day appointments as this will be the real test of the impact of the sms campaign."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "is_same_day = (df['scheduled_day'].dt.date == df['appointment_day'].dt.date) & (df['scheduled_day'].dt.month == df['appointment_day'].dt.month)\n",
        "same_day_appts = df[is_same_day == True]\n",
        "same_day_appts_count = same_day_appts.value_counts().sum()\n",
        "# Non-same day appointments\n",
        "not_same_day_appts = df[is_same_day == False]\n",
        "\n",
        "print(f\"Number of appointments scheduled on the same day: {same_day_appts_count}\")\n",
        "print(f\"Percentage of appointments scheduled on the same day: {(same_day_appts_count / df.shape[0]) * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_same_day_appts_count = not_same_day_appts.value_counts().sum()\n",
        "print(f\"Number of appointments scheduled on different days: {not_same_day_appts_count}\")\n",
        "not_same_day_appts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 9))\n",
        "sns.countplot(x='sms_received', hue='no_show', data=not_same_day_appts, palette='pastel')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('SMS Received', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.title('Attendance comparison based on SMS Reception', fontsize=14)\n",
        "\n",
        "# Show plot\n",
        "plt.legend(title='Attended', labels=['Yes', 'No'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After filtering out same-day appointments, the new analysis revealed that patients who did not receive an SMS had a show percentage of 70.55% and a no-show percentage of 29.45%. Those who received an SMS showed a slight increase in attendance, with a show percentage of 72.43% and a no-show percentage of 27.57%. This suggests that, for non-same-day appointments, receiving an SMS has a modest positive impact on attendance, improving the show rate by approximately 2% compared to those who did not receive an SMS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the percentages of show and noshow instances for each category of SMS reception for non-same day appointments\n",
        "perc_sms_show = not_same_day_appts.groupby('sms_received')['no_show'].value_counts(normalize=True)[:, 'No'] * 100\n",
        "perc_sms_noshow = not_same_day_appts.groupby('sms_received')['no_show'].value_counts(normalize=True)[:, 'Yes'] * 100\n",
        "\n",
        "# Prepare data for printing as a table\n",
        "table_data = []\n",
        "for sms_received, show_percentage, noshow_percentage in zip(perc_sms_show.index, perc_sms_show.values, perc_sms_noshow.values):\n",
        "    table_data.append([sms_received, show_percentage, noshow_percentage])\n",
        "\n",
        "# Print table headers\n",
        "print(\"SMS Received | Show Percentage | Noshow Percentage\")\n",
        "# Print table rows\n",
        "for row in table_data:\n",
        "    print(\"{:<12} | {:<15.4f}% | {:<15.4f}%\".format(*row))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attendance comparison by Handicap\n",
        "\n",
        "Based on the analysis of attendance comparison based on the level of handicap, we observe varying trends. The majority of appointments involve patients with no reported handicap, comprising approximately 97.97% of the dataset. Among these appointments, the no-show rate is 20.24%, indicating a moderate but notable proportion of missed appointments. Interestingly, appointments involving patients with a reported handicap level of 1 or 2 exhibit slightly lower no-show rates compared to those with no reported handicap, suggesting a potential correlation between a mild level of handicap and increased appointment attendance. However, caution is warranted in interpreting these findings due to the relatively small sample sizes of patients with higher levels of handicap (levels 3 and 4), which may not be representative. Further investigation with larger datasets or stratified analyses by handicap severity may provide deeper insights into the relationship between handicap level and appointment attendance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the counts for show and no-show for each handicap level\n",
        "handicap_attendance_counts = df.groupby(['handicap', 'no_show']).size().unstack().fillna(0)\n",
        "\n",
        "# Plot the bar chart\n",
        "handicap_attendance_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['lightgreen', 'lightcoral'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Handicap Level')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Attendance Comparison Based on Handicap')\n",
        "plt.legend(title='No-show', loc='upper right', labels=['Show', 'No-show'])\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the total count per handicap level\n",
        "handicap_total_counts = df.groupby('handicap').size()\n",
        "\n",
        "\n",
        "handicap_attendance_counts = df.groupby(['handicap', 'no_show']).size().unstack().fillna(0)\n",
        "\n",
        "handicap_attendance_percentages = handicap_attendance_counts.div(handicap_total_counts, axis=0) * 100\n",
        "\n",
        "handicap_summary = pd.DataFrame({\n",
        "    'Handicap Level': handicap_total_counts.index,\n",
        "    'Total Count': handicap_total_counts.values,\n",
        "    'Show Count': handicap_attendance_counts['No'].values,\n",
        "    'Noshow Count': handicap_attendance_counts['Yes'].values,\n",
        "    'Show Percentage': handicap_attendance_percentages['No'].values,\n",
        "    'Noshow Percentage': handicap_attendance_percentages['Yes'].values\n",
        "})\n",
        "\n",
        "# Print the summary table\n",
        "print(handicap_summary.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attendance Comparison based on Scholarship status\n",
        "\n",
        "We observe that the majority of patients without scholarship status attended their appointments, with an attendance rate of 80.19%. Conversely, patients with scholarship status had a slightly lower attendance rate of 76.26%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the counts for show and no-show for each scholarship status\n",
        "scholarship_attendance_counts = df.groupby(['scholarship', 'no_show']).size().unstack()\n",
        "\n",
        "# Plot the bar chart\n",
        "scholarship_attendance_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['lightgreen', 'lightcoral'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Scholarship Status')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Attendance Comparison Based on Scholarship Status')\n",
        "plt.legend(title='No-show', loc='upper right', labels=['Show', 'No-show'])\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.show()\n",
        "# #does neighbourhood affect the attendance?\n",
        "# plt.figure(figsize=(20,13))\n",
        "# df['scholarship'][show].value_counts().plot(kind='bar', color = 'blue', label = 'show')\n",
        "# df['scholarship'][no_show].value_counts().plot(kind='bar', color = 'red', label = 'noShow')\n",
        "# plt.legend()\n",
        "# plt.title('comparison according to Neighbourhood')\n",
        "# plt.xlabel('Neighbourhood')\n",
        "# plt.ylabel('Patient No.')\n",
        "\n",
        "scholarship_attendance_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.groupby('scholarship').size().sum())\n",
        "df.groupby('scholarship')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the total count per scholarship status\n",
        "scholarship_total_counts = df.groupby('scholarship').size()\n",
        "scholarship_attendance_perc = scholarship_attendance_counts.div(scholarship_total_counts, axis=0) * 100\n",
        "\n",
        "scholarship_summary = pd.DataFrame({\n",
        "    'Scholarship Status': scholarship_total_counts.index,\n",
        "    'Total Count': scholarship_total_counts.values,\n",
        "    'Show Count': scholarship_attendance_counts['No'].values,\n",
        "    'Noshow Count': scholarship_attendance_counts['Yes'].values,\n",
        "    'Show Percentage': scholarship_attendance_perc['No'].values,\n",
        "    'Noshow Percentage': scholarship_attendance_perc['Yes'].values\n",
        "})\n",
        "\n",
        "print(scholarship_summary.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attendance comparison based on Neighborhood\n",
        "\n",
        "The variability in the percentages of attendance per neighbourhood shows neighbourhood has a strong effect on attendance, perhaps more than other features explored in this analysis. This may be the factor that contributes most to attendance and this might need to be investigated further, although that is beyond the scope of the data analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#does neighbourhood affect the attendance?\n",
        "plt.figure(figsize=(20,10))\n",
        "df['neighbourhood'][show].value_counts().plot(kind='bar', color = 'lightgreen', label = 'show')\n",
        "df['neighbourhood'][no_show].value_counts().plot(kind='bar', color = 'lightcoral', label = 'no show')\n",
        "plt.legend()\n",
        "plt.title('comparison according to Neighbourhood')\n",
        "plt.xlabel('Neighbourhood')\n",
        "plt.ylabel('Patient No.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by neighborhood and no_show, calculate counts\n",
        "neighborhood_counts = df.groupby(['neighbourhood', 'no_show']).size().unstack().fillna(0)\n",
        "\n",
        "# Calculate total count per neighborhood\n",
        "total_counts = neighborhood_counts.sum(axis=1)\n",
        "\n",
        "# Sort neighborhoods by total count in descending order\n",
        "sorted_neighborhood_counts = neighborhood_counts.loc[total_counts.sort_values(ascending=False).index]\n",
        "\n",
        "# Calculate percentages\n",
        "neighborhood_percentages = (sorted_neighborhood_counts.div(sorted_neighborhood_counts.sum(axis=1), axis=0) * 100).round(2)\n",
        "\n",
        "# Prepare the data for printing\n",
        "data = {\n",
        "    'Neighborhood': sorted_neighborhood_counts.index,\n",
        "    'Total Count': sorted_neighborhood_counts.sum(axis=1),\n",
        "    'Show Count': sorted_neighborhood_counts['No'],\n",
        "    'Noshow Count': sorted_neighborhood_counts['Yes'],\n",
        "    'Show Percentage': neighborhood_percentages['No'],\n",
        "    'Noshow Percentage': neighborhood_percentages['Yes']\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "result_df = pd.DataFrame(data)\n",
        "\n",
        "# Print the table\n",
        "print(result_df.head(15).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Imbalance Investigation\n",
        "There is a significant imbalance between the classes as over 88k patients attended their appointments versus over 22k missing their appointments. A similar imbalance still appears even after filtering out same-day appointments as it was already known that 35% of the appointments were same-day appointments which majorly were shows (No in no_show class). This occurence must be considered during data modeling. This also means the metric for model quality may not be accuracy and might be other metrics like F1 Score and ROC AUC. Another possible technique that can be implemented could be Random Undersampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['no_show'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Investigate the class imbalance in the dataset (no-shows vs shows) and plot the distribution on one bar chart\n",
        "\n",
        "sns.countplot(x='no_show', data=df, palette='pastel')\n",
        "plt.title(\"Classes distribution\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check class imbalance if same day appointments are removed  and plot the distribution on one bar chart\n",
        "sns.countplot(x='no_show', data=not_same_day_appts, palette='pastel')\n",
        "plt.title(\"Classes distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Ratio of shows to no-shows\n",
        "no_show_ratio = df['no_show'].value_counts(normalize=True)['Yes'] / df['no_show'].value_counts(normalize=True)['No']\n",
        "no_show_ratio = round(no_show_ratio, 2)\n",
        "print(f'Ratio of shows to no-shows: {no_show_ratio.as_integer_ratio()[1]}:{no_show_ratio.as_integer_ratio()[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Modelling\n",
        "As we proceed with the data modeling stage, two data modeling techniques were chosen for predicting if a patient will miss their appointments or not. The model techniques used are:\n",
        "1. Logistic Regression Classifier\n",
        "2. Neural Network Classifier\n",
        "\n",
        "This is the order of steps that will be followed:\n",
        "1. Perform feature engineering.\n",
        "    - Convert categorical features to numerical. This can be done via one hot encoding.\n",
        "    - Create new features as needed.\n",
        "    - Perform feature selection.\n",
        "3. Split dataset into training, validation and testing sets.\n",
        "4. Design and train the models on the training set and hypertune with validation set.\n",
        "5. Evaluate the model's performance via accuracy, F1, confusion matrix and ROC AUC results from testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Convert categorical features to numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfm = df.copy(deep=True)\n",
        "dfm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows where the values of the handicap column are greater than 1\n",
        "dfm.drop(dfm[dfm['handicap'] > 1].index, inplace=True)\n",
        "dfm.reset_index(drop=True, inplace=True)\n",
        "dfm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Male and Female numerical columns from gender column\n",
        "dfm['is_male'] = dfm['gender'].map({'M': 1, 'F': 0})\n",
        "\n",
        "# Convert the target variable column\n",
        "dfm['no_show'] = dfm['no_show'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Drop the gender column\n",
        "dfm.drop(columns=['gender'], inplace=True, axis=1)\n",
        "print(dfm.tail())\n",
        "dfm.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfm.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Design Thought Process\n",
        "\n",
        "## Knowledge Summary\n",
        "This is what we know from the dataset.\n",
        "1. The target variable is significantly imbalanced with more patients attending vs no shows.\n",
        "2. The dataset has a mix of categorical and numerical features. But we have converted the categorical features to numerical.\n",
        "3. The dataset has no missing or duplicate values.\n",
        "4. We have used frequency encoding for the neighbourhood column which is a high cardinality column.\n",
        "5. We have normalized the age column.\n",
        "6. We have evaluated the correlations between the features and the target variable using the Chi-Square test and the Pearson correlation coefficient.\n",
        "7. From the correlation tests, we can see the top 5 features that are most related to the target variable are: sms_received, hypertension, scholarship, neighbourhood_encoded, and age_scaled.\n",
        "\n",
        "## Next Steps\n",
        "1. We will split the dataset into training, validation, and test sets. We will use 70% of the data for training, 15% for validation, and 15% for testing\n",
        "2. We will need to handle the class imbalance in the training set by using Random undersampling.\n",
        "3. We will use a Random Forest Classifier to build the model.\n",
        "4. We will evaluate the model using the validation set.\n",
        "5. We will fine-tune the model using GridSearchCV.\n",
        "6. We will evaluate the model using the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We have neighbourhood column with high cardinality. Using one-hot encoding will increase the number of columns significantly. Therefore, we can use frequency encoding to encode the neighbourhood column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Selection\n",
        "\n",
        "Before modeling, we need to select what features may contribute the most information gain to the model, i.e correlates with the target variables. From the dataset, most of the features are binary, including the target variables. For categorical input features with categorical output/target, a well-known method for determining the correlation is called Chi-Square Test. However, we have converted the neighbourhood and age features to non-binary numerical features. For these columns, we may need to apply a different correlation discovery technique called Pearson Correlation Coefficient.\n",
        "\n",
        "## Chi Square \n",
        "I will select the best 3 features with the highest importance from the results of conducting the Chi Square. This works by choosing the three features with the highest chi values and lowest p values.\n",
        "\n",
        "# Pearson Correlation Coefficient\n",
        "As there are only 2 input features,adding to the previously selected 3 features from the Chi-Square test gives 5 features which is not too much for the model. So after visualization, we can add both features as inputs for the models and evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import chi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfm.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = dfm.drop(columns=['no_show', 'patient_id', 'scheduled_day', 'appointment_day', 'neighbourhood','age'], axis=1)\n",
        "y = dfm['no_show']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chi_scores = chi2(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chi_values = pd.Series(chi_scores[0], index=X.columns)\n",
        "chi_values.sort_values(ascending=False, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Higher the p-value, the more the feature is independent of the target variable\n",
        "\n",
        "p_values = pd.Series(chi_scores[1], index=X.columns)\n",
        "p_values.sort_values(ascending=False, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print Chi and P-values for each feature\n",
        "print(\"Chi and P-values for each feature:\")\n",
        "# Print the table\n",
        "print(pd.concat([chi_values, p_values], axis=1, keys=['Chi', 'P-value']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Higher the chi-squared value, the more the feature is related to the target variable\n",
        "# Plot the chi-squared values using a bar chart. Give a chart title of 'Chi value importance for each feature'\n",
        "chi_values.plot(kind='bar', figsize=(12, 6), color='skyblue')\n",
        "plt.title('Chi value importance for each feature')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the p-values using a bar chart. Give a chart title of 'P-value importance for each feature'\n",
        "p_values.plot(kind='bar', figsize=(12, 6), color='lightcoral')\n",
        "plt.title('P-value importance for each feature')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sns.set_theme(style=\"white\")\n",
        "# non_binary_cols = ['neighbourhood_encoded', 'age_scaled', 'no_show']\n",
        "# corr = dfm[non_binary_cols].corr()  # Create the pearson correlation metrix object\n",
        "\n",
        "# fig, ax = plt.subplots() # create the figure\n",
        "\n",
        "# sns.heatmap(corr, annot=True, cmap='Greens', annot_kws={'rotation':45}) # Draw the heatmap\n",
        "\n",
        "# plt.title(\"Correlation Matrix\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation of the Logistic Regression Model on the Validation Set\n",
        "\n",
        "#### 1. Model Performance Metrics\n",
        "\n",
        "**Accuracy**: 0.63\n",
        "\n",
        "The accuracy of the model is 63%, which indicates that 63% of the predictions made by the model are correct. However, accuracy alone is not a sufficient metric to evaluate the performance of the model, especially in the presence of class imbalance.\n",
        "\n",
        "**Confusion Matrix**:\n",
        "```\n",
        "                 Predicted Negative  Predicted Positive\n",
        "Actual Negative                5543                2492\n",
        "Actual Positive                1483                1235\n",
        "```\n",
        "\n",
        "**Classification Report**:\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.79      0.69      0.74      8035\n",
        "           1       0.33      0.45      0.38      2718\n",
        "\n",
        "    accuracy                           0.63     10753\n",
        "   macro avg       0.56      0.57      0.56     10753\n",
        "weighted avg       0.67      0.63      0.65     10753\n",
        "```\n",
        "\n",
        "**ROC AUC Score**: 0.58\n",
        "\n",
        "#### 2. Detailed Analysis\n",
        "\n",
        "**Precision, Recall, and F1-Score**:\n",
        "- **Precision for No-Show (Class 1)**: 0.33\n",
        "  - This indicates that when the model predicts a patient will miss an appointment, it is correct only 33% of the time.\n",
        "- **Recall for No-Show (Class 1)**: 0.45\n",
        "  - This indicates that the model correctly identifies 45% of the actual missed appointments.\n",
        "- **F1-Score for No-Show (Class 1)**: 0.38\n",
        "  - The F1-score is the harmonic mean of precision and recall, providing a balance between the two. A score of 0.38 indicates poor performance in predicting missed appointments.\n",
        "\n",
        "**ROC AUC Score**: 0.58\n",
        "- The ROC AUC score of 0.58 indicates that the model has limited ability to distinguish between patients who will attend and those who will not. A score closer to 1 indicates better discrimination, while a score closer to 0.5 suggests random guessing.\n",
        "\n",
        "**Confusion Matrix**:\n",
        "- **True Positives (TP)**: 1235 (Patients predicted as No-Show who actually missed)\n",
        "- **False Positives (FP)**: 2492 (Patients predicted as No-Show who actually attended)\n",
        "- **True Negatives (TN)**: 5543 (Patients predicted as Show who actually attended)\n",
        "- **False Negatives (FN)**: 1483 (Patients predicted as Show who actually missed)\n",
        "\n",
        "The model's confusion matrix reveals that:\n",
        "- It correctly predicts 1235 out of 2718 actual missed appointments.\n",
        "- It incorrectly predicts 1483 missed appointments as attended.\n",
        "- It incorrectly predicts 2492 attended appointments as missed.\n",
        "- It correctly predicts 5543 out of 8035 actual attended appointments.\n",
        "\n",
        "#### 3. Relation to Business Understanding and Goals\n",
        "\n",
        "The primary business goal is to identify patients who are likely to miss their appointments so that interventions (such as reminders) can be targeted to improve attendance rates. Given this goal, the performance of the logistic regression model can be evaluated as follows:\n",
        "\n",
        "**Recall for No-Show (45%)**:\n",
        "- Recall is crucial in this context because we want to identify as many no-show patients as possible. A recall of 45% means that more than half of the patients who miss their appointments are not being identified by the model, which is suboptimal for the business goal.\n",
        "\n",
        "**Precision for No-Show (33%)**:\n",
        "- Precision is important to ensure that resources (like reminders) are not wasted on patients who are likely to attend. With a precision of 33%, the model is generating a large number of false positives, meaning many patients who are predicted to miss their appointments actually attend. This inefficiency could lead to wasted efforts and resources.\n",
        "\n",
        "**F1-Score (38%)**:\n",
        "- The F1-score, which balances precision and recall, is low. This suggests that the overall effectiveness of the model in identifying no-shows while minimizing false predictions is not satisfactory.\n",
        "\n",
        "**ROC AUC Score (0.58)**:\n",
        "- An ROC AUC score of 0.58 indicates the model's poor performance in distinguishing between the two classes (show vs. no-show). It is only slightly better than random guessing, highlighting the need for improvement.\n",
        "\n",
        "#### 4. Recommendations for Improvement\n",
        "\n",
        "To achieve the business goal of accurately identifying patients who are likely to miss their appointments, the following steps can be taken to improve the model:\n",
        "\n",
        "1. **Feature Engineering**: Create additional features that may better capture the patterns associated with missed appointments, such as interaction terms or derived features from existing ones (e.g., days between scheduling and appointment).\n",
        "\n",
        "2. **Addressing Class Imbalance**: While the training set was undersampled, consider using techniques like Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic samples for the minority class or adjusting class weights in the logistic regression model to give more importance to the minority class.\n",
        "\n",
        "3. **Model Tuning**: Perform hyperparameter tuning using techniques like GridSearchCV or RandomizedSearchCV to find the optimal parameters for the logistic regression model.\n",
        "\n",
        "4. **Ensemble Methods**: Explore ensemble methods such as Random Forest, Gradient Boosting, or XGBoost, which might capture more complex patterns in the data.\n",
        "\n",
        "5. **Threshold Adjustment**: Fine-tune the classification threshold to find a balance that improves recall for the no-show class without drastically reducing precision.\n",
        "\n",
        "By addressing these areas, the model can be enhanced to better meet the business objective of predicting no-show appointments, thereby enabling more effective interventions and improving overall attendance rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from utility.utils import preprocess_to_modelling_pipeline\n",
        "log_reg_model = LogisticRegression(C=10, random_state=42, max_iter=1000)\n",
        "preprocess_to_modelling_pipeline(df=dfm, lvl1_test_size=0.3, lvl2_test_size=0.5, random_state=42, model=log_reg_model, target_col_label='no_show', selected_cols=['age', 'scholarship', 'hypertension', 'sms_received', 'neighbourhood'], pick_results='validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature engineering is a critical step in improving the performance of machine learning models, particularly when initial attempts with other methods have not yielded significant improvements. Below are some proposed new features that could potentially enhance the model's ability to predict no-shows, along with reasons for their inclusion:\n",
        "\n",
        "### Proposed New Features\n",
        "\n",
        "1. **Days Between Scheduling and Appointment**\n",
        "   - **Reason**: The time gap between when an appointment is scheduled and when it is actually held could influence the likelihood of a no-show. Longer gaps may lead to more no-shows due to changes in patients' schedules or forgotten appointments.\n",
        "   \n",
        "2. **Previous No-Shows**\n",
        "   - **Reason**: Patients with a history of no-shows are more likely to miss future appointments. This feature can capture the no-show behavior of patients.\n",
        "   ```python\n",
        "   df['previous_no_shows'] = df.groupby('patient_id')['no_show'].transform('sum')\n",
        "   ```\n",
        "\n",
        "3. **Cumulative Appointments**\n",
        "   - **Reason**: The total number of past appointments a patient has scheduled can provide insights into their reliability and commitment to attending appointments.\n",
        "   ```python\n",
        "   df['cumulative_appointments'] = df.groupby('patient_id').cumcount() + 1\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Create new column for the days between scheduled and appointment days\n",
        "\n",
        "dfm['days_between'] = (dfm['appointment_day'] - dfm['scheduled_day']).dt.days\n",
        "\n",
        "dfm['days_between'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Handle rows where days_between is negative as appointment day cannot come before the scheduled day.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfm.query(\"days_between < 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**There are a lot of negatives. After inspecting, we can see that the scheduled_day has the time included while the appointment day doesnt. This leads to the scheduled_day being ahead by time. This can be resolved by removing time from the scheduled day.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the time part of the scheduled_day column\n",
        "dfm['scheduled_day'] = dfm['scheduled_day'].dt.normalize()\n",
        "dfm['days_between'] = (dfm['appointment_day'] - dfm['scheduled_day']).dt.days\n",
        "dfm.query(\"days_between < 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows where the days_between column is negative\n",
        "dfm.drop(dfm[dfm['days_between'] < 0].index, inplace=True)\n",
        "dfm.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new column to indicate if a patient has missed a previous appointment\n",
        "# dfm['previous_no_show'] = dfm.groupby('patient_id')['no_show'].shift(1).fillna(0).astype(int)\n",
        "dfm['previous_no_shows'] = dfm.groupby('patient_id')['no_show'].transform('sum')\n",
        "dfm['cumulative_appointments'] = dfm.groupby('patient_id').cumcount() + 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check pearson correlation for the new features via plotting a heatmap\n",
        "sns.set_theme(style=\"white\")\n",
        "non_binary_cols = ['days_between', 'previous_no_shows', 'cumulative_appointments', 'no_show', 'age']\n",
        "corr = dfm[non_binary_cols].corr()  # Create the pearson correlation metrix object\n",
        "\n",
        "fig, ax = plt.subplots() # create the figure\n",
        "\n",
        "sns.heatmap(corr, annot=True, cmap='Greens', annot_kws={'rotation':45}) # Draw the heatmap\n",
        "\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Evaluation of the Logistic Regression Model\n",
        "\n",
        "#### Goal and Business Understanding\n",
        "The goal is to predict patients who are likely to miss their appointments (i.e., `no_show` = 1) so that intervention measures can be taken to improve attendance rates. The key is to minimize missed appointments, which can help improve the efficiency of healthcare services and ensure better utilization of resources.\n",
        "\n",
        "#### Model Performance Metrics\n",
        "\n",
        "1. **Accuracy: 0.88**\n",
        "   - This means that 88% of the predictions made by the model are correct. While this is a high accuracy, it is not the sole metric to consider, especially with class imbalance.\n",
        "\n",
        "2. **Confusion Matrix:**\n",
        "   ```\n",
        "                     Predicted Negative  Predicted Positive\n",
        "   Actual Negative                6747                1329\n",
        "   Actual Positive                   0                2677\n",
        "   ```\n",
        "   - **True Negatives (TN):** 6747 - Patients who were predicted to attend and did attend.\n",
        "   - **False Positives (FP):** 1329 - Patients who were predicted to miss but attended.\n",
        "   - **False Negatives (FN):** 0 - Patients who were predicted to attend but missed.\n",
        "   - **True Positives (TP):** 2677 - Patients who were predicted to miss and did miss.\n",
        "\n",
        "3. **Classification Report:**\n",
        "   ```\n",
        "                 precision    recall  f1-score   support\n",
        "\n",
        "              0       1.00      0.84      0.91      8076\n",
        "              1       0.67      1.00      0.80      2677\n",
        "\n",
        "       accuracy                           0.88     10753\n",
        "      macro avg       0.83      0.92      0.86     10753\n",
        "   weighted avg       0.92      0.88      0.88     10753\n",
        "   ```\n",
        "   - **Precision (no_show=0):** 1.00\n",
        "     - This means that out of all the patients predicted to attend, 100% actually attended.\n",
        "   - **Recall (no_show=0):** 0.84\n",
        "     - Out of all the patients who actually attended, 84% were correctly predicted.\n",
        "   - **Precision (no_show=1):** 0.67\n",
        "     - Out of all the patients predicted to miss, 67% actually missed.\n",
        "   - **Recall (no_show=1):** 1.00\n",
        "     - Out of all the patients who actually missed, 100% were correctly predicted.\n",
        "   - **F1-score:**\n",
        "     - A harmonic mean of precision and recall. For no_show=1, it is 0.80, which indicates a good balance.\n",
        "\n",
        "4. **ROC AUC Score: 0.94**\n",
        "   - The ROC AUC score of 0.94 indicates excellent discriminatory ability of the model to distinguish between patients who will attend and those who will miss their appointments.\n",
        "\n",
        "### Interpretation and Insights\n",
        "\n",
        "1. **High Recall for no_show=1 (Missed Appointments):**\n",
        "   - The recall for predicting missed appointments is perfect (1.00), meaning the model correctly identifies all patients who will miss their appointments. This is crucial for the business goal since it ensures no potential no-show is missed by the model.\n",
        "\n",
        "2. **Moderate Precision for no_show=1:**\n",
        "   - The precision for predicting missed appointments is 0.67, indicating that 33% of the predicted no-shows will actually attend their appointments. This can lead to unnecessary interventions for some patients.\n",
        "\n",
        "3. **No False Negatives:**\n",
        "   - There are no false negatives, meaning the model does not incorrectly classify any actual no-show as a show. This is highly desirable because it avoids missing out on patients who need intervention.\n",
        "\n",
        "4. **Impact on Business:**\n",
        "   - **High Recall:** Ensures that almost all patients who are likely to miss their appointments are identified, allowing for targeted interventions such as reminders or rescheduling.\n",
        "   - **Moderate Precision:** While there is a risk of some false positives, the impact is less critical than false negatives in this scenario. However, reducing false positives could lead to more efficient resource allocation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the pipeline with the new features\n",
        "log_model2 = LogisticRegression(C=10, random_state=42, max_iter=1000, solver='liblinear')\n",
        "preprocess_to_modelling_pipeline(df=dfm, lvl1_test_size=0.3, lvl2_test_size=0.5, random_state=42, model=log_model2, target_col_label='no_show', selected_cols=['age', 'scholarship', 'hypertension', 'sms_received', 'neighbourhood', 'days_between', 'previous_no_shows', 'cumulative_appointments'], pick_results='validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**After testing with diverse split distributions for the dataset splits, one can observe that the model has definitely improved with the addition of these two new features. The accuracy improved up to 86.6% on average as well as the very important recall. The ROC AUC score of 0.91 shows the model prediction capacity is very distinguishable from random guessing.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the pipeline multiple times with different random states and randomized lvl1 and lvl2 test sizes\n",
        "random_lvl1_test_sizes = [0.8, 0.7, 0.6, 0.5]\n",
        "random_random_states = [0, 42]\n",
        "val_accuracy_avg = []\n",
        "val_recall_avg = {'0': [], '1': []}\n",
        "val_precision_avg = {'0': [], '1': []}\n",
        "val_f1_avg = {'0': [], '1': []}\n",
        "val_roc_auc_avg = []\n",
        "\n",
        "\n",
        "for lvl1_test_size in random_lvl1_test_sizes:    \n",
        "    for random_state in random_random_states:\n",
        "        log_model3 = LogisticRegression(C=10, random_state=random_state, max_iter=1000, solver='liblinear')\n",
        "        acc_score, conf_mtx, cls_report, roc_auc_score, fpr,tpr, thres,_,_,_,_,_,_,_  = preprocess_to_modelling_pipeline(df=dfm, lvl1_test_size=lvl1_test_size, lvl2_test_size=0.5, random_state=random_state, model=log_model3, target_col_label='no_show', selected_cols=['age', 'scholarship', 'hypertension', 'sms_received', 'neighbourhood', 'days_between', 'previous_no_shows', 'cumulative_appointments'], pick_results='validation', plot=True, cls_report_as_dict=True)\n",
        "        log_model3 = None\n",
        "        val_accuracy_avg.append(acc_score)\n",
        "        val_recall_avg['0'].append(cls_report['0']['recall'])\n",
        "        val_recall_avg['1'].append(cls_report['1']['recall'])\n",
        "        val_precision_avg['0'].append(cls_report['0']['precision'])\n",
        "        val_precision_avg['1'].append(cls_report['1']['precision'])\n",
        "        val_f1_avg['0'].append(cls_report['0']['f1-score'])\n",
        "        val_f1_avg['1'].append(cls_report['1']['f1-score'])\n",
        "        val_roc_auc_avg.append(roc_auc_score)\n",
        "\n",
        "        \n",
        "        \n",
        "# Declaratively perform computations to get the average values using numpy\n",
        "def get_avg_values(values):\n",
        "    return {key: np.mean(value) for key, value in values.items()}\n",
        "#Apply the function to the dictionaries\n",
        "val_recall_avg = get_avg_values(val_recall_avg)\n",
        "val_precision_avg = get_avg_values(val_precision_avg)\n",
        "val_f1_avg = get_avg_values(val_f1_avg)\n",
        "\n",
        "# Print the average values\n",
        "print(f\"Average Accuracy: {np.mean(val_accuracy_avg)}\")\n",
        "print(f\"Average Recall: {val_recall_avg}\")\n",
        "print(f\"Average Precision: {val_precision_avg}\")\n",
        "print(f\"Average F1-score: {val_f1_avg}\")\n",
        "print(f\"Average AUC: {np.mean(val_roc_auc_avg)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Random Forest Classifier Model.\n",
        "Implementing this model without the new features yielded poorer performance than Logistic Regression without the features.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Without the days_between, previous_no_shows and cumulative_appointments columns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "preprocess_to_modelling_pipeline(df=dfm, lvl1_test_size=0.3, lvl2_test_size=0.5, random_state=42, model=rf_model, target_col_label='no_show', selected_cols=['age', 'scholarship', 'hypertension', 'sms_received', 'neighbourhood'], pick_results='validation', plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Performance is much higher with the new features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# With the days_between, previous_no_shows and cumulative_appointments columns\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "preprocess_to_modelling_pipeline(df=dfm, lvl1_test_size=0.3, lvl2_test_size=0.5, random_state=42, model=rf_model, target_col_label='no_show', selected_cols=['age', 'scholarship', 'hypertension', 'sms_received', 'neighbourhood', 'days_between', 'previous_no_shows', 'cumulative_appointments'], pick_results='validation', plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation of Model Performance:\n",
        "\n",
        "**Business Goal:** To predict patient no-shows accurately so that the clinic can take preemptive actions to reduce missed appointments.\n",
        "\n",
        "**Logistic Regression:**\n",
        "- **Accuracy:** 87.94%\n",
        "- **Precision:** 69.43%\n",
        "- **Recall:** 91.07%\n",
        "- **F1 Score:** 78.79%\n",
        "- **ROC AUC:** 0.9465\n",
        "\n",
        "**Random Forest:**\n",
        "- **Accuracy:** 88.71%\n",
        "- **Precision:** 73.78%\n",
        "- **Recall:** 83.90%\n",
        "- **F1 Score:** 78.51%\n",
        "- **ROC AUC:** 0.9450\n",
        "\n",
        "### Detailed Interpretation:\n",
        "\n",
        "The logistic regression model achieves a high recall of 91.07%, meaning it effectively identifies most patients who will not show up for their appointments. This high recall is essential in a clinical setting where the goal is to minimize no-shows by predicting and intervening with those likely to miss their appointments. The model’s accuracy and F1 score also indicate good overall performance, with an ROC AUC of 0.9465, signifying strong discriminative ability between patients who will show and those who won't.\n",
        "\n",
        "On the other hand, the random forest model achieves a slightly higher accuracy of 88.71% and a precision of 73.78%, meaning it is better at reducing false positives—patients predicted to no-show but actually show up. This model provides a balanced approach with strong overall performance and an ROC AUC of 0.9450. The recall of 83.90%, while slightly lower than logistic regression, is still substantial, ensuring a significant number of no-shows are accurately predicted.\n",
        "\n",
        "### Business Implications:\n",
        "\n",
        "The logistic regression model’s high recall makes it suitable for scenarios where catching as many no-shows as possible is critical. This approach maximizes the clinic’s ability to intervene with patients likely to miss appointments, improving overall attendance rates. However, the random forest model offers a balanced trade-off between precision and recall, which could be beneficial to minimize unnecessary interventions, reducing potential resource wastage.\n",
        "\n",
        "Given the statistically significant performance difference, the random forest model’s slight edge in accuracy and precision might be preferred for a balanced and reliable prediction strategy. However, if the priority is to ensure almost all no-shows are captured, logistic regression might be more suitable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "from utility.utils import preprocessing_pipeline\n",
        "\n",
        "# Define evaluation metrics\n",
        "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
        "n_splits = 10\n",
        "# Choose cross-validation method\n",
        "k_folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Select models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(C=10, random_state=42, max_iter=2500),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "X_train, y_train = preprocessing_pipeline(df=dfm, selected_cols=['age', 'scholarship', 'hypertension', 'sms_received', 'neighbourhood', 'days_between', 'previous_no_shows', 'cumulative_appointments'], target_col_label='no_show')\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_train.head())\n",
        "print(y_train.head())\n",
        "# Perform cross-validation and record results\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    model_results = {}\n",
        "    for metric in scoring:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=k_folds, scoring=metric)\n",
        "        model_results[metric] = scores.mean()\n",
        "    results[model_name] = model_results\n",
        "\n",
        "# Print results\n",
        "for model_name, model_result in results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    for metric, score in model_result.items():\n",
        "        print(f\"{metric}: {score}\")\n",
        "\n",
        "# Perform statistical tests\n",
        "metric_of_interest = 'accuracy'\n",
        "model1_scores = [results['Logistic Regression'][metric_of_interest] for _ in range(n_splits)]\n",
        "model2_scores = [results['Random Forest'][metric_of_interest] for _ in range(n_splits)]\n",
        "t_statistic, p_value = ttest_rel(model1_scores, model2_scores)\n",
        "\n",
        "# Evaluate significance\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Performance difference is statistically significant.\")\n",
        "else:\n",
        "    print(\"Performance difference is not statistically significant.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
